{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jeJWounsqhpY"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import _pickle as cPickle\n",
    "import gzip\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "D7VXd_waqhpl"
   },
   "outputs": [],
   "source": [
    "# utility functions\n",
    "\n",
    "def one_hot_encoded(y, num_class):\n",
    "    n = y.shape[0]\n",
    "    onehot = np.zeros((n, num_class), dtype=\"int32\")\n",
    "    for i in range(n):\n",
    "        idx = y[i]\n",
    "        onehot[i][idx] = 1\n",
    "    return onehot\n",
    "\n",
    "\n",
    "def check_accuracy(y_true, y_pred):\n",
    "    return np.mean(y_pred == y_true)  # both are not one hot encoded\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    # софтмакс сделан более устойчивым с помощью вычитания np.max\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "# l2 regularization\n",
    "def l2_reg(layers, lam=0.001):\n",
    "    reg_loss = 0.0\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, 'W'):\n",
    "            reg_loss += 0.5 * lam * np.sum(layer.W * layer.W)\n",
    "    return reg_loss\n",
    "\n",
    "\n",
    "# l2 regularization grad\n",
    "def delta_l2_reg(layers, grads, lam=0.001):\n",
    "    for layer, grad in zip(layers, reversed(grads)):\n",
    "        if hasattr(layer, 'W'):\n",
    "            grad[0] += lam * layer.W\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gepenqShqhpn"
   },
   "outputs": [],
   "source": [
    "# функция для проверки грандиентов\n",
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    Adopted from https://github.com/ddtm/dl-course/\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aNH3LoYVqhpq"
   },
   "outputs": [],
   "source": [
    "# dout - upstream gradient\n",
    "# gradInput - upstream_gradient*local_gradient\n",
    "# forward = ReLU(X)\n",
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X, mode):\n",
    "        self.X = X\n",
    "        return np.maximum(X, 0)\n",
    "\n",
    "    def backward(self, dout, mode):\n",
    "        self.gradInput = dout.copy()\n",
    "        self.gradInput[self.X <= 0] = 0\n",
    "        return self.gradInput, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsQb5bWBqhps",
    "outputId": "51a99084-e40b-4311-e101-3dcf57717a33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "relu = ReLU()\n",
    "f = lambda x: relu.forward(x, mode='train').sum(axis=1).sum()\n",
    "res = f(points)\n",
    "numeric_grads = eval_numerical_gradient(f, points)\n",
    "print(numeric_grads)\n",
    "inp_grad = np.ones(shape=(10, 12))\n",
    "grads = relu.backward(inp_grad, mode='train')[0]\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "t7FTFIqDqhpw"
   },
   "outputs": [],
   "source": [
    "# Xavier init - input dispersion is similar to output dispersion\n",
    "# for convergence\n",
    "# \n",
    "class Linear():\n",
    "    def __init__(self, in_size, out_size):\n",
    "        # Xavier init\n",
    "        self.W = np.random.randn(in_size, out_size) / np.sqrt(in_size + out_size/ 2.)\n",
    "        self.b = np.zeros((1, out_size))\n",
    "        self.params = [self.W, self.b]\n",
    "        self.gradW = None\n",
    "        self.gradB = None\n",
    "        self.gradInput = None\n",
    "\n",
    "    def forward(self, X, mode):\n",
    "        self.X = X\n",
    "        out = self.X.dot(self.W) + self.b\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout, mode):\n",
    "        self.gradW = self.X.T.dot(dout)\n",
    "        self.gradB = np.mean(dout, axis=0)\n",
    "        self.gradInput = dout.dot(self.W.T)\n",
    "        return self.gradInput, [self.gradW, self.gradB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-83NwDBqhpy",
    "outputId": "4ff7326d-1771-42af-b34e-c2353629f20a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.51130580e-04  6.14180038e-01  2.11436234e-01  2.22470347e-01\n",
      "  -1.84192922e-01  7.54657708e-01  8.95621202e-02  1.29161640e-01\n",
      "   6.29682943e-01 -2.48729555e-01 -2.31738277e-01 -1.00379699e+00]\n",
      " [-7.51130580e-04  6.14180038e-01  2.11436234e-01  2.22470347e-01\n",
      "  -1.84192922e-01  7.54657708e-01  8.95621202e-02  1.29161640e-01\n",
      "   6.29682943e-01 -2.48729555e-01 -2.31738277e-01 -1.00379699e+00]\n",
      " [-7.51130580e-04  6.14180038e-01  2.11436234e-01  2.22470347e-01\n",
      "  -1.84192922e-01  7.54657708e-01  8.95621202e-02  1.29161640e-01\n",
      "   6.29682943e-01 -2.48729555e-01 -2.31738277e-01 -1.00379699e+00]\n",
      " [-7.51130580e-04  6.14180038e-01  2.11436234e-01  2.22470347e-01\n",
      "  -1.84192922e-01  7.54657708e-01  8.95621202e-02  1.29161640e-01\n",
      "   6.29682943e-01 -2.48729555e-01 -2.31738277e-01 -1.00379699e+00]\n",
      " [-7.51130580e-04  6.14180038e-01  2.11436234e-01  2.22470347e-01\n",
      "  -1.84192922e-01  7.54657708e-01  8.95621202e-02  1.29161640e-01\n",
      "   6.29682943e-01 -2.48729555e-01 -2.31738277e-01 -1.00379699e+00]\n",
      " [-7.51130558e-04  6.14180038e-01  2.11436234e-01  2.22470347e-01\n",
      "  -1.84192922e-01  7.54657708e-01  8.95621202e-02  1.29161640e-01\n",
      "   6.29682943e-01 -2.48729555e-01 -2.31738277e-01 -1.00379699e+00]\n",
      " [-7.51130558e-04  6.14180038e-01  2.11436234e-01  2.22470347e-01\n",
      "  -1.84192922e-01  7.54657708e-01  8.95621202e-02  1.29161640e-01\n",
      "   6.29682943e-01 -2.48729555e-01 -2.31738277e-01 -1.00379699e+00]\n",
      " [-7.51130558e-04  6.14180038e-01  2.11436234e-01  2.22470347e-01\n",
      "  -1.84192922e-01  7.54657708e-01  8.95621202e-02  1.29161640e-01\n",
      "   6.29682943e-01 -2.48729555e-01 -2.31738277e-01 -1.00379699e+00]\n",
      " [-7.51130580e-04  6.14180038e-01  2.11436234e-01  2.22470347e-01\n",
      "  -1.84192922e-01  7.54657708e-01  8.95621202e-02  1.29161640e-01\n",
      "   6.29682943e-01 -2.48729555e-01 -2.31738277e-01 -1.00379699e+00]\n",
      " [-7.51130547e-04  6.14180038e-01  2.11436234e-01  2.22470347e-01\n",
      "  -1.84192922e-01  7.54657708e-01  8.95621202e-02  1.29161640e-01\n",
      "   6.29682943e-01 -2.48729555e-01 -2.31738277e-01 -1.00379699e+00]]\n"
     ]
    }
   ],
   "source": [
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "linear = Linear(12, 5)\n",
    "f = lambda x: linear.forward(x, mode='train').sum(axis=1).sum()\n",
    "res = f(points)\n",
    "numeric_grads = eval_numerical_gradient(f, points)\n",
    "print(numeric_grads)\n",
    "inp_grad = np.ones(shape=(10, 5))\n",
    "grads = linear.backward(inp_grad, mode='train')[0]\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Iz8ZiCXMqhp0"
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(object):\n",
    "    def forward(self, X, y):\n",
    "        # y can only be 1 or 0\n",
    "        self.m = y.shape[0]\n",
    "        self.p = softmax(X)\n",
    "        # -log(p_true)\n",
    "        cross_entropy = -np.log(self.p[range(self.m), y])\n",
    "        loss = np.sum(cross_entropy) / self.m\n",
    "        return loss\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        dx = self.p.copy()\n",
    "        dx[range(self.m), y] -= 1\n",
    "        dx /= self.m\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPOGFjZuqhp2"
   },
   "source": [
    "## NN implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LYkYJWpuqhp4"
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, loss_func=CrossEntropyLoss(), mode = 'train'):\n",
    "        self.layers = []\n",
    "        self.params = []\n",
    "        self.loss_func = loss_func\n",
    "        self.grads = []\n",
    "        self.mode = mode\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        self.params.append(layer.params)\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X, self.mode)\n",
    "        return X\n",
    "\n",
    "    def backward(self, dout):\n",
    "        self.clear_grad_param()\n",
    "        for layer in reversed(self.layers):\n",
    "            dout, grad = layer.backward(dout, self.mode)\n",
    "            self.grads.append(grad)\n",
    "        return self.grads\n",
    "\n",
    "    def train_step(self, X, y):\n",
    "        out = self.forward(X)\n",
    "        loss = self.loss_func.forward(out, y)\n",
    "        dout = self.loss_func.backward(out, y)\n",
    "        loss += l2_reg(self.layers)\n",
    "        grads = self.backward(dout)\n",
    "        grads = delta_l2_reg(self.layers, grads)\n",
    "        return loss, grads\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.forward(X)\n",
    "        return np.argmax(softmax(X), axis=1)\n",
    "\n",
    "    def dispGradParam():\n",
    "        print(self.grads)\n",
    "    \n",
    "    def clear_grad_param(self):\n",
    "        self.grads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "4JyB2D61qhp6"
   },
   "outputs": [],
   "source": [
    "# SGD with momentum\n",
    "def update(velocity, params, grads, learning_rate=0.001, mu=0.9):\n",
    "    for v, p, g, in zip(velocity, params, reversed(grads)):\n",
    "        for i in range(len(g)):\n",
    "            v[i] = mu * v[i] + learning_rate * g[i]\n",
    "            p[i] -= v[i]\n",
    "\n",
    "# get minibatches\n",
    "def minibatch(X, y, minibatch_size):\n",
    "    n = X.shape[0]\n",
    "    minibatches = []\n",
    "    X, y = shuffle(X, y)\n",
    "\n",
    "    for i in range(0, n , minibatch_size):\n",
    "        X_batch = X[i:i + minibatch_size, ...]\n",
    "        y_batch = y[i:i + minibatch_size, ...]\n",
    "\n",
    "        minibatches.append((X_batch, y_batch))\n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "pOfONQ0AqhqA"
   },
   "outputs": [],
   "source": [
    "def train(net, X_train, y_train, minibatch_size, epoch, learning_rate, mu=0.9,\n",
    "          verbose=True, X_val=None, y_val=None, nesterov=True):\n",
    "    val_loss_epoch = []\n",
    "    minibatches = minibatch(X_train, y_train, minibatch_size)\n",
    "    minibatches_val = minibatch(X_val, y_val, minibatch_size)\n",
    "\n",
    "    c = 0 \n",
    "    for i in range(epoch):\n",
    "        loss_batch = []\n",
    "        val_loss_batch = []\n",
    "        velocity = []\n",
    "        for param_layer in net.params:\n",
    "            p = [np.zeros_like(param) for param in list(param_layer)]\n",
    "            velocity.append(p)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Epoch {0}\".format(i + 1))\n",
    "\n",
    "        # iterate over mini batches\n",
    "        # for X_mini, y_mini in tqdm(minibatches):\n",
    "        for X_mini, y_mini in minibatches:\n",
    "            loss, grads = net.train_step(X_mini, y_mini)\n",
    "            loss_batch.append(loss)\n",
    "            update(velocity, net.params, grads,\n",
    "                            learning_rate=learning_rate, mu=mu)\n",
    "\n",
    "        # for X_mini_val, y_mini_val in tqdm(minibatches_val):\n",
    "        for X_mini_val, y_mini_val in minibatches_val:\n",
    "            # валидация, можно сделать просто форвард\n",
    "            val_loss, _ = net.train_step(X_mini, y_mini)\n",
    "            val_loss_batch.append(val_loss)\n",
    "\n",
    "        # accuracy of model at end of epoch after all mini batch updates   \n",
    "        if verbose:\n",
    "            m_train = X_train.shape[0]\n",
    "            m_val = X_val.shape[0]\n",
    "            y_train_pred = np.array([], dtype=\"int64\")\n",
    "            y_val_pred = np.array([], dtype=\"int64\")\n",
    "\n",
    "            for i in range(0, m_train, minibatch_size):\n",
    "                X_tr = X_train[i:i + minibatch_size, : ]\n",
    "                y_tr = y_train[i:i + minibatch_size, ]\n",
    "                y_train_pred = np.append(y_train_pred, net.predict(X_tr))\n",
    "\n",
    "            for i in range(0, m_val, minibatch_size):\n",
    "                X_va = X_val[i:i + minibatch_size, : ]\n",
    "                y_va = y_val[i:i + minibatch_size, ]\n",
    "                y_val_pred = np.append(y_val_pred, net.predict(X_va))\n",
    "\n",
    "            train_acc = check_accuracy(y_train, y_train_pred)\n",
    "            val_acc = check_accuracy(y_val, y_val_pred)\n",
    "\n",
    "            mean_train_loss = sum(loss_batch) / float(len(loss_batch))\n",
    "            mean_val_loss = sum(val_loss_batch) / float(len(val_loss_batch))\n",
    "\n",
    "            # early stopping with patience = 5 on val loss\n",
    "            if len(val_loss_epoch) == 0:\n",
    "                val_loss_epoch.append(mean_val_loss)\n",
    "            else:\n",
    "                for j in val_loss_epoch[-5:]:\n",
    "                    if mean_val_loss > j:\n",
    "                        c += 1\n",
    "                    else:\n",
    "                        c = 0\n",
    "                if c > 5:\n",
    "                    print('Early stopping')\n",
    "                    return net\n",
    "                else:\n",
    "                    c = 0\n",
    "                    val_loss_epoch.append(mean_val_loss)    \n",
    "\n",
    "            print(\"Loss = {0} | Training Accuracy = {1} | \" \\\n",
    "                  \"Val Loss = {2} | Val Accuracy = {3}\".format(\n",
    "                mean_train_loss, train_acc, mean_val_loss, val_acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fad8x1_IqhqB",
    "outputId": "5e1e620b-8bcb-4076-8b1e-f401c5d7db24",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4)\n",
      "(4, 4)\n",
      "Epoch 1\n",
      "Loss = 0.7959107968358331 | Training Accuracy = 0.5 | Val Loss = 0.6946165950029364 | Val Accuracy = 0.5\n",
      "Epoch 2\n",
      "Loss = 0.6946165950029364 | Training Accuracy = 0.5 | Val Loss = 0.6870352521997738 | Val Accuracy = 0.5\n",
      "Epoch 3\n",
      "Loss = 0.6870352521997738 | Training Accuracy = 0.5 | Val Loss = 0.6835600640754544 | Val Accuracy = 0.5\n",
      "Epoch 4\n",
      "Loss = 0.6835600640754544 | Training Accuracy = 0.5 | Val Loss = 0.6807154117016951 | Val Accuracy = 0.5\n",
      "Epoch 5\n",
      "Loss = 0.6807154117016951 | Training Accuracy = 0.5 | Val Loss = 0.6782148086215168 | Val Accuracy = 0.5\n",
      "Epoch 6\n",
      "Loss = 0.6782148086215168 | Training Accuracy = 0.5 | Val Loss = 0.675986346354122 | Val Accuracy = 0.5\n",
      "Epoch 7\n",
      "Loss = 0.675986346354122 | Training Accuracy = 0.5 | Val Loss = 0.673924271067078 | Val Accuracy = 0.5\n",
      "Epoch 8\n",
      "Loss = 0.673924271067078 | Training Accuracy = 0.5 | Val Loss = 0.671963404668465 | Val Accuracy = 0.5\n",
      "Epoch 9\n",
      "Loss = 0.671963404668465 | Training Accuracy = 0.5 | Val Loss = 0.6700804379570554 | Val Accuracy = 0.5\n",
      "Epoch 10\n",
      "Loss = 0.6700804379570554 | Training Accuracy = 0.5 | Val Loss = 0.6682573273429206 | Val Accuracy = 0.5\n",
      "Epoch 11\n",
      "Loss = 0.6682573273429206 | Training Accuracy = 0.5 | Val Loss = 0.6664807073323934 | Val Accuracy = 0.5\n",
      "Epoch 12\n",
      "Loss = 0.6664807073323934 | Training Accuracy = 0.5 | Val Loss = 0.6647402098541031 | Val Accuracy = 0.5\n",
      "Epoch 13\n",
      "Loss = 0.6647402098541031 | Training Accuracy = 0.5 | Val Loss = 0.6630548740718495 | Val Accuracy = 0.5\n",
      "Epoch 14\n",
      "Loss = 0.6630548740718495 | Training Accuracy = 0.5 | Val Loss = 0.6613869394421716 | Val Accuracy = 0.5\n",
      "Epoch 15\n",
      "Loss = 0.6613869394421716 | Training Accuracy = 0.5 | Val Loss = 0.6597113428010671 | Val Accuracy = 0.5\n",
      "Epoch 16\n",
      "Loss = 0.6597113428010671 | Training Accuracy = 0.5 | Val Loss = 0.658050569902964 | Val Accuracy = 0.5\n",
      "Epoch 17\n",
      "Loss = 0.658050569902964 | Training Accuracy = 0.5 | Val Loss = 0.6563890656520571 | Val Accuracy = 0.5\n",
      "Epoch 18\n",
      "Loss = 0.6563890656520571 | Training Accuracy = 0.5 | Val Loss = 0.6547370614503052 | Val Accuracy = 0.5\n",
      "Epoch 19\n",
      "Loss = 0.6547370614503052 | Training Accuracy = 0.5 | Val Loss = 0.6530991340458764 | Val Accuracy = 0.5\n",
      "Epoch 20\n",
      "Loss = 0.6530991340458764 | Training Accuracy = 0.5 | Val Loss = 0.6514571481497479 | Val Accuracy = 0.5\n",
      "Epoch 21\n",
      "Loss = 0.6514571481497479 | Training Accuracy = 0.5 | Val Loss = 0.649815192888267 | Val Accuracy = 0.5\n",
      "Epoch 22\n",
      "Loss = 0.649815192888267 | Training Accuracy = 0.5 | Val Loss = 0.6479612877495518 | Val Accuracy = 0.5\n",
      "Epoch 23\n",
      "Loss = 0.6479612877495518 | Training Accuracy = 0.5 | Val Loss = 0.6462764641069514 | Val Accuracy = 0.5\n",
      "Epoch 24\n",
      "Loss = 0.6462764641069514 | Training Accuracy = 0.5 | Val Loss = 0.644605817884788 | Val Accuracy = 0.5\n",
      "Epoch 25\n",
      "Loss = 0.644605817884788 | Training Accuracy = 0.5 | Val Loss = 0.6429355985149778 | Val Accuracy = 0.5\n",
      "Epoch 26\n",
      "Loss = 0.6429355985149778 | Training Accuracy = 0.5 | Val Loss = 0.6412728596544504 | Val Accuracy = 0.5\n",
      "Epoch 27\n",
      "Loss = 0.6412728596544504 | Training Accuracy = 0.5 | Val Loss = 0.6395986264645269 | Val Accuracy = 0.5\n",
      "Epoch 28\n",
      "Loss = 0.6395986264645269 | Training Accuracy = 0.5 | Val Loss = 0.6379398150580076 | Val Accuracy = 0.5\n",
      "Epoch 29\n",
      "Loss = 0.6379398150580076 | Training Accuracy = 0.5 | Val Loss = 0.6362650362940144 | Val Accuracy = 0.5\n",
      "Epoch 30\n",
      "Loss = 0.6362650362940144 | Training Accuracy = 0.5 | Val Loss = 0.634576375713697 | Val Accuracy = 0.5\n",
      "Epoch 31\n",
      "Loss = 0.634576375713697 | Training Accuracy = 0.5 | Val Loss = 0.6328907570608613 | Val Accuracy = 0.5\n",
      "Epoch 32\n",
      "Loss = 0.6328907570608613 | Training Accuracy = 0.5 | Val Loss = 0.6312017222753387 | Val Accuracy = 0.5\n",
      "Epoch 33\n",
      "Loss = 0.6312017222753387 | Training Accuracy = 0.5 | Val Loss = 0.6295088492011831 | Val Accuracy = 0.5\n",
      "Epoch 34\n",
      "Loss = 0.6295088492011831 | Training Accuracy = 0.5 | Val Loss = 0.6278117673838259 | Val Accuracy = 0.5\n",
      "Epoch 35\n",
      "Loss = 0.6278117673838259 | Training Accuracy = 0.5 | Val Loss = 0.6261101298077457 | Val Accuracy = 0.5\n",
      "Epoch 36\n",
      "Loss = 0.6261101298077457 | Training Accuracy = 0.5 | Val Loss = 0.6244036071173559 | Val Accuracy = 0.5\n",
      "Epoch 37\n",
      "Loss = 0.6244036071173559 | Training Accuracy = 0.5 | Val Loss = 0.6226918836735857 | Val Accuracy = 0.5\n",
      "Epoch 38\n",
      "Loss = 0.6226918836735857 | Training Accuracy = 0.5 | Val Loss = 0.6209746544450695 | Val Accuracy = 0.5\n",
      "Epoch 39\n",
      "Loss = 0.6209746544450695 | Training Accuracy = 0.5 | Val Loss = 0.6192516225391942 | Val Accuracy = 0.5\n",
      "Epoch 40\n",
      "Loss = 0.6192516225391942 | Training Accuracy = 0.5 | Val Loss = 0.6175224972385449 | Val Accuracy = 0.5\n",
      "Epoch 41\n",
      "Loss = 0.6175224972385449 | Training Accuracy = 0.5 | Val Loss = 0.615786992436698 | Val Accuracy = 0.5\n",
      "Epoch 42\n",
      "Loss = 0.615786992436698 | Training Accuracy = 0.5 | Val Loss = 0.6140448253896051 | Val Accuracy = 0.5\n",
      "Epoch 43\n",
      "Loss = 0.6140448253896051 | Training Accuracy = 0.5 | Val Loss = 0.6122957157165025 | Val Accuracy = 0.5\n",
      "Epoch 44\n",
      "Loss = 0.6122957157165025 | Training Accuracy = 0.5 | Val Loss = 0.6105393845982602 | Val Accuracy = 0.5\n",
      "Epoch 45\n",
      "Loss = 0.6105393845982602 | Training Accuracy = 0.5 | Val Loss = 0.6087755541321258 | Val Accuracy = 0.5\n",
      "Epoch 46\n",
      "Loss = 0.6087755541321258 | Training Accuracy = 0.5 | Val Loss = 0.6070043639828396 | Val Accuracy = 0.5\n",
      "Epoch 47\n",
      "Loss = 0.6070043639828396 | Training Accuracy = 0.5 | Val Loss = 0.6052256753708769 | Val Accuracy = 0.5\n",
      "Epoch 48\n",
      "Loss = 0.6052256753708769 | Training Accuracy = 0.5 | Val Loss = 0.6034375171243166 | Val Accuracy = 0.5\n",
      "Epoch 49\n",
      "Loss = 0.6034375171243166 | Training Accuracy = 0.5 | Val Loss = 0.6016413364254828 | Val Accuracy = 0.5\n",
      "Epoch 50\n",
      "Loss = 0.6016413364254828 | Training Accuracy = 0.5 | Val Loss = 0.5998487027584387 | Val Accuracy = 0.5\n",
      "Epoch 51\n",
      "Loss = 0.5998487027584387 | Training Accuracy = 0.5 | Val Loss = 0.598028768170882 | Val Accuracy = 0.5\n",
      "Epoch 52\n",
      "Loss = 0.598028768170882 | Training Accuracy = 0.5 | Val Loss = 0.5961976546772088 | Val Accuracy = 0.5\n",
      "Epoch 53\n",
      "Loss = 0.5961976546772088 | Training Accuracy = 0.5 | Val Loss = 0.5943622460189079 | Val Accuracy = 0.5\n",
      "Epoch 54\n",
      "Loss = 0.5943622460189079 | Training Accuracy = 0.5 | Val Loss = 0.592516926957145 | Val Accuracy = 0.5\n",
      "Epoch 55\n",
      "Loss = 0.592516926957145 | Training Accuracy = 0.5 | Val Loss = 0.5906613436581181 | Val Accuracy = 0.5\n",
      "Epoch 56\n",
      "Loss = 0.5906613436581181 | Training Accuracy = 0.5 | Val Loss = 0.5887951922909478 | Val Accuracy = 0.5\n",
      "Epoch 57\n",
      "Loss = 0.5887951922909478 | Training Accuracy = 0.5 | Val Loss = 0.5869181711255208 | Val Accuracy = 0.5\n",
      "Epoch 58\n",
      "Loss = 0.5869181711255208 | Training Accuracy = 0.5 | Val Loss = 0.5850299793991158 | Val Accuracy = 0.5\n",
      "Epoch 59\n",
      "Loss = 0.5850299793991158 | Training Accuracy = 0.5 | Val Loss = 0.5831303166691589 | Val Accuracy = 0.5\n",
      "Epoch 60\n",
      "Loss = 0.5831303166691589 | Training Accuracy = 0.5 | Val Loss = 0.5812188823133112 | Val Accuracy = 0.5\n",
      "Epoch 61\n",
      "Loss = 0.5812188823133112 | Training Accuracy = 0.5 | Val Loss = 0.5792953751494455 | Val Accuracy = 0.5\n",
      "Epoch 62\n",
      "Loss = 0.5792953751494455 | Training Accuracy = 0.5 | Val Loss = 0.5773594931508577 | Val Accuracy = 0.5\n",
      "Epoch 63\n",
      "Loss = 0.5773594931508577 | Training Accuracy = 0.5 | Val Loss = 0.5754109332371579 | Val Accuracy = 0.5\n",
      "Epoch 64\n",
      "Loss = 0.5754109332371579 | Training Accuracy = 0.5 | Val Loss = 0.5734493911257189 | Val Accuracy = 0.5\n",
      "Epoch 65\n",
      "Loss = 0.5734493911257189 | Training Accuracy = 0.5 | Val Loss = 0.5714749445663888 | Val Accuracy = 0.5\n",
      "Epoch 66\n",
      "Loss = 0.5714749445663888 | Training Accuracy = 0.5 | Val Loss = 0.5694884361195653 | Val Accuracy = 0.5\n",
      "Epoch 67\n",
      "Loss = 0.5694884361195653 | Training Accuracy = 0.5 | Val Loss = 0.5674879907552299 | Val Accuracy = 0.5\n",
      "Epoch 68\n",
      "Loss = 0.5674879907552299 | Training Accuracy = 0.5 | Val Loss = 0.5654904429245752 | Val Accuracy = 0.5\n",
      "Epoch 69\n",
      "Loss = 0.5654904429245752 | Training Accuracy = 0.5 | Val Loss = 0.5634552139848077 | Val Accuracy = 0.5\n",
      "Epoch 70\n",
      "Loss = 0.5634552139848077 | Training Accuracy = 0.5 | Val Loss = 0.5614066488773939 | Val Accuracy = 0.5\n",
      "Epoch 71\n",
      "Loss = 0.5614066488773939 | Training Accuracy = 0.5 | Val Loss = 0.559346100369096 | Val Accuracy = 0.5\n",
      "Epoch 72\n",
      "Loss = 0.559346100369096 | Training Accuracy = 0.5 | Val Loss = 0.5572701149017684 | Val Accuracy = 0.5\n",
      "Epoch 73\n",
      "Loss = 0.5572701149017684 | Training Accuracy = 0.5 | Val Loss = 0.5551783680761242 | Val Accuracy = 0.5\n",
      "Epoch 74\n",
      "Loss = 0.5551783680761242 | Training Accuracy = 0.5 | Val Loss = 0.5530705404495216 | Val Accuracy = 0.5\n",
      "Epoch 75\n",
      "Loss = 0.5530705404495216 | Training Accuracy = 0.5 | Val Loss = 0.550946312535001 | Val Accuracy = 0.5\n",
      "Epoch 76\n",
      "Loss = 0.550946312535001 | Training Accuracy = 0.5 | Val Loss = 0.5488053646763049 | Val Accuracy = 0.5\n",
      "Epoch 77\n",
      "Loss = 0.5488053646763049 | Training Accuracy = 0.5 | Val Loss = 0.5466473770834537 | Val Accuracy = 0.5\n",
      "Epoch 78\n",
      "Loss = 0.5466473770834537 | Training Accuracy = 0.5 | Val Loss = 0.5444720299125014 | Val Accuracy = 0.5\n",
      "Epoch 79\n",
      "Loss = 0.5444720299125014 | Training Accuracy = 0.5 | Val Loss = 0.5422790033870794 | Val Accuracy = 0.5\n",
      "Epoch 80\n",
      "Loss = 0.5422790033870794 | Training Accuracy = 0.5 | Val Loss = 0.5400679779551824 | Val Accuracy = 0.5\n",
      "Epoch 81\n",
      "Loss = 0.5400679779551824 | Training Accuracy = 0.5 | Val Loss = 0.5378386344768515 | Val Accuracy = 0.5\n",
      "Epoch 82\n",
      "Loss = 0.5378386344768515 | Training Accuracy = 0.5 | Val Loss = 0.535590654439638 | Val Accuracy = 0.5\n",
      "Epoch 83\n",
      "Loss = 0.535590654439638 | Training Accuracy = 0.5 | Val Loss = 0.5333237201996928 | Val Accuracy = 0.5\n",
      "Epoch 84\n",
      "Loss = 0.5333237201996928 | Training Accuracy = 0.5 | Val Loss = 0.5310375152470619 | Val Accuracy = 0.5\n",
      "Epoch 85\n",
      "Loss = 0.5310375152470619 | Training Accuracy = 0.5 | Val Loss = 0.5287317244943113 | Val Accuracy = 0.5\n",
      "Epoch 86\n",
      "Loss = 0.5287317244943113 | Training Accuracy = 1.0 | Val Loss = 0.5264060345880192 | Val Accuracy = 1.0\n",
      "Epoch 87\n",
      "Loss = 0.5264060345880192 | Training Accuracy = 1.0 | Val Loss = 0.5240601342429818 | Val Accuracy = 1.0\n",
      "Epoch 88\n",
      "Loss = 0.5240601342429818 | Training Accuracy = 1.0 | Val Loss = 0.5216937145992085 | Val Accuracy = 1.0\n",
      "Epoch 89\n",
      "Loss = 0.5216937145992085 | Training Accuracy = 1.0 | Val Loss = 0.5193064696019507 | Val Accuracy = 1.0\n",
      "Epoch 90\n",
      "Loss = 0.5193064696019507 | Training Accuracy = 1.0 | Val Loss = 0.5168980964051351 | Val Accuracy = 1.0\n",
      "Epoch 91\n",
      "Loss = 0.5168980964051351 | Training Accuracy = 1.0 | Val Loss = 0.5144682957986588 | Val Accuracy = 1.0\n",
      "Epoch 92\n",
      "Loss = 0.5144682957986588 | Training Accuracy = 1.0 | Val Loss = 0.5120179618975129 | Val Accuracy = 1.0\n",
      "Epoch 93\n",
      "Loss = 0.5120179618975129 | Training Accuracy = 1.0 | Val Loss = 0.5095480101702387 | Val Accuracy = 1.0\n",
      "Epoch 94\n",
      "Loss = 0.5095480101702387 | Training Accuracy = 1.0 | Val Loss = 0.507059237854074 | Val Accuracy = 1.0\n",
      "Epoch 95\n",
      "Loss = 0.507059237854074 | Training Accuracy = 1.0 | Val Loss = 0.504543179731481 | Val Accuracy = 1.0\n",
      "Epoch 96\n",
      "Loss = 0.504543179731481 | Training Accuracy = 1.0 | Val Loss = 0.5020008749976969 | Val Accuracy = 1.0\n",
      "Epoch 97\n",
      "Loss = 0.5020008749976969 | Training Accuracy = 1.0 | Val Loss = 0.49943506763013445 | Val Accuracy = 1.0\n",
      "Epoch 98\n",
      "Loss = 0.49943506763013445 | Training Accuracy = 1.0 | Val Loss = 0.49664194228909647 | Val Accuracy = 1.0\n",
      "Epoch 99\n",
      "Loss = 0.49664194228909647 | Training Accuracy = 1.0 | Val Loss = 0.4940289231110272 | Val Accuracy = 1.0\n",
      "Epoch 100\n",
      "Loss = 0.4940289231110272 | Training Accuracy = 1.0 | Val Loss = 0.49154741749216885 | Val Accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Get preprocessed training and validation data\n",
    "\n",
    "X_train = np.array([\n",
    "    [1, 2, 1, 2],\n",
    "    [2, 4, 2, 4],\n",
    "    [2, 1, 2, 1],\n",
    "    [4, 2, 4, 2],\n",
    "])\n",
    "\n",
    "y_train = np.array([0, 1, 0, 1])\n",
    "X_val = X_train.copy()\n",
    "y_val = y_train.copy()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "# define neural net\n",
    "model = NN()\n",
    "\n",
    "# add some layers\n",
    "model.add_layer(Linear(4, 100))\n",
    "model.add_layer(ReLU())\n",
    "model.add_layer(Linear(100, 100))\n",
    "model.add_layer(ReLU())\n",
    "model.add_layer(Linear(100, 2))\n",
    "\n",
    "model = train(model, X_train , y_train, minibatch_size=4, epoch=100,\n",
    "           learning_rate=0.1, X_val=X_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nasalrYhqhqC"
   },
   "source": [
    "## Mnist training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GBnh0t5wqhqD"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "b5WLrrZbqhqE"
   },
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0W1H-iRHqhqF"
   },
   "outputs": [],
   "source": [
    "y = y.astype(np.int32)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, \n",
    "                                                  test_size=0.25,\n",
    "                                                  shuffle=True,\n",
    "                                                  random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Nb7XAVXnqhqG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "87iNgx8uqhqH",
    "outputId": "8c47f275-3d21-46d9-ddb9-39448ea2ecfc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD3CAYAAADmIkO7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAHq0lEQVR4nO3dT4jU5x3H8edpU+OfELamrTbGP0gOMacWFUHIzXqoJBIptkGQTY4xxeZkQnoxpSFFKBp78CJF8GLJJSAecggEpIQekhXEBrGLwUCJWZEcQjUx+fXSUkKdZ8zOjvOZ8fW6BPzym3kC++Yn+3V+U7uuK0Ce74z6AMDtiRNCiRNCiRNCiRNCiRNCiRNCiXOC1Fp/VWv9e63181rrP2qtT4z6TMzffaM+AAuj1vqzUsofSim/LKX8rZTy49GeiEFV/0JoMtRa/1pKOd513fFRn4WF4a+1E6DW+t1SyqZSyg9rrZdqrR/XWv9Ua10y6rMxf+KcDCtKKd8rpfyilPJEKeUnpZSfllJ+O8IzMSBxToZ//ee/R7uu+2fXdXOllD+WUn4+wjMxIHFOgK7rrpdSPi6l+AXCBBHn5PhzKeXXtdYf1Vq/X0r5TSnl9GiPxCCsUibH70opPyilXCyl3Cil/KWU8vuRnoiBWKVAKH+thVDihFDihFDihFDN39bWWv22CIas67p6uz9354RQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQnr43Zvbv39+cHz58uDm/ePFic/7aa6/1nJ04caJ5LQvLnRNCiRNCiRNCiRNCiRNCiRNCNb8rxaMxh2PRokU9Z/3WFTt27GjOly1bNq8z/dfMzEzP2caNGwd6bW7PozFhzIgTQokTQokTQokTQokTQokTQvnI2BD02zW2dpW7d+9e6ON8KxcuXBjp+/M/7pwQSpwQSpwQSpwQSpwQSpwQSpwQyp5zHh544IHm/MiRI8359PT0Ap7mmz744IPm/NChQ83522+/vZDHYQDunBBKnBBKnBBKnBBKnBBKnBBKnBDKc2vn4ejRo835888/P7T3/uyzz5rzDRs2NOeffPLJQh6HBeC5tTBmxAmhxAmhxAmhxAmhxAmhfGTsNjZt2tSc79q1a2jvffPmzeb8qaeeas6tSiaHOyeEEieEEieEEieEEieEEieEEieEuif3nC+//HJz/uKLLzbnDz300EDv39plbt++vXnt2bNnB3pvxoc7J4QSJ4QSJ4QSJ4QSJ4QSJ4QSJ4S6Jx+NefXq1eZ80D1mv8dXtj6TaY957/FoTBgz4oRQ4oRQ4oRQ4oRQ4oRQ4oRQE/t5zscff7zn7P777x/ota9du9acP/PMM825XSZ3wp0TQokTQokTQokTQokTQokTQokTQk3snnNubq7n7NatWwO99pIlSwa6Hu6EOyeEEieEEieEEieEEieEEieEmthVSuvxl1999dVAr7106dLm/MEHHxzo9VOtW7euOZ+ammrOt23b1pz3+/rDltnZ2eb82LFjzXm/9dr58+e/9ZkG5c4JocQJocQJocQJocQJocQJocQJoSb2KwCffPLJnrNTp041r+336MxLly415/32eVeuXGnOh2nx4sXN+UsvvdRztnfv3ua1a9eundeZEty8ebM53717d8/Z6dOnB3pvXwEIY0acEEqcEEqcEEqcEEqcEEqcEGpiP8+5YcOGnrNhfwXgKPeYW7Zsac4PHDjQnO/cuXMhjzM2+v1MPPbYYz1ng+45e3HnhFDihFDihFDihFDihFDihFDihFATu+ccplWrVjXnrZ1YKaV8+OGHC3mcb3jzzTeb84cffnho7z2o999/v+fszJkzzWufe+655jz5/7sXd04IJU4IJU4IJU4IJU4IJU4IJU4INbF7zpmZmZ6zGzduNK/t92zXRx55pDl/9tlnm/N+n6kcV++++25z3u+zol9++WXP2dNPP9289osvvmjO++n3M3Hu3LmBXn8+3DkhlDghlDghlDghlDghlDgh1MSuUlq/Wv/666+H+t7Lly9vztevX99zNjs727x2enq6OV+xYkVzPojjx4835ydPnmzOt2/f3py/+uqrPWePPvpo89r77mv/KPf72sZ+Z/voo4+a82Fw54RQ4oRQ4oRQ4oRQ4oRQ4oRQ4oRQteu63sNaew/H2Hvvvdecb968eajvf/ny5Z6zq1evNq9duXJlc75mzZr5HOmO9PtY1vXr15vzYe5g+9m3b19zfuzYsbt0kv/XdV293Z+7c0IocUIocUIocUIocUIocUIocUKoe3LPuXr16ub8xIkTzXm/PejSpUu/9ZkYzDvvvNOc79mzpznvt18eJntOGDPihFDihFDihFDihFDihFDihFD35J5zUG+99VZzvnXr1ua833Nt71Vzc3M9Z6+//nrz2n7PzP3000/ndaa7wZ4Txow4IZQ4IZQ4IZQ4IZQ4IZRVyhBMTU015y+88ELP2cGDBxf4NHfPrVu3mvNXXnmlOX/jjTd6zvo9lnOcWaXAmBEnhBInhBInhBInhBInhBInhLLnhBGz54QxI04IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IJU4IVbuuG/UZgNtw54RQ4oRQ4oRQ4oRQ4oRQ4oRQ/wbDXF7c28ytCwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize data\n",
    "def vis(img, label):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(label)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "vis_idx = 0\n",
    "vis(X_train[vis_idx].reshape(-1, 28), y_train[vis_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFUz9i2qqhqJ",
    "outputId": "6c2fc05d-7db7-4091-c267-6e30cfc6a087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52500, 784)\n",
      "(17500, 784)\n",
      "Epoch 1\n",
      "Loss = 1.0217574101943865 | Training Accuracy = 0.9141714285714285 | Val Loss = 0.22804140677344528 | Val Accuracy = 0.9050857142857143\n",
      "Epoch 2\n",
      "Loss = 0.33443694735053436 | Training Accuracy = 0.9350666666666667 | Val Loss = 0.2416318797769541 | Val Accuracy = 0.9258285714285714\n",
      "Epoch 3\n",
      "Loss = 0.2837600135772083 | Training Accuracy = 0.9448761904761904 | Val Loss = 0.30293059231717484 | Val Accuracy = 0.9326857142857143\n",
      "Epoch 4\n",
      "Loss = 0.2575712480143273 | Training Accuracy = 0.9491238095238095 | Val Loss = 0.3030371171636381 | Val Accuracy = 0.9369714285714286\n",
      "Epoch 5\n",
      "Loss = 0.23762706126592784 | Training Accuracy = 0.9548761904761904 | Val Loss = 0.25639394172039 | Val Accuracy = 0.9412571428571429\n",
      "Epoch 6\n",
      "Loss = 0.22322566955060824 | Training Accuracy = 0.9591047619047619 | Val Loss = 0.2573845947720768 | Val Accuracy = 0.9438285714285715\n",
      "Epoch 7\n",
      "Loss = 0.21071704601441113 | Training Accuracy = 0.9618857142857142 | Val Loss = 0.2368416190839533 | Val Accuracy = 0.9450857142857143\n",
      "Epoch 8\n",
      "Loss = 0.20146578739344137 | Training Accuracy = 0.9657714285714286 | Val Loss = 0.1942310132016754 | Val Accuracy = 0.9484\n",
      "Epoch 9\n",
      "Loss = 0.19317937008068498 | Training Accuracy = 0.968 | Val Loss = 0.15954249796759715 | Val Accuracy = 0.9488\n",
      "Epoch 10\n",
      "Loss = 0.1860841127647368 | Training Accuracy = 0.9696571428571429 | Val Loss = 0.14794350445081036 | Val Accuracy = 0.9490857142857143\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "# define neural net\n",
    "model = NN()\n",
    "\n",
    "# add some layers\n",
    "model.add_layer(Linear(X_train.shape[1], 100))\n",
    "model.add_layer(ReLU())\n",
    "model.add_layer(Linear(100, 100))\n",
    "model.add_layer(ReLU())\n",
    "model.add_layer(Linear(100, 10))\n",
    "\n",
    "model = train(model, X_train , y_train, minibatch_size=128, epoch=10,\n",
    "           learning_rate=0.001, X_val=X_val, y_val=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "id": "bPLSr2-CqhqL",
    "outputId": "d0da6bf4-11a2-4728-f019-96353e7093d3"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize prediction \n",
    "vis_idx = 1\n",
    "pred = model.predict(X_val[vis_idx])\n",
    "vis(X_val[vis_idx].reshape(-1, 28), pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "id": "VODz-x2iakIr",
    "outputId": "44701fca-e6a0-44a8-fdb6-7d4eb49ee43e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1da44291f0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize first layer mean weights\n",
    "t = model.layers[0].W.mean(axis=1).reshape(28, 28)\n",
    "plt.matshow(t)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "seminar2_task.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
